\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{float}
% \usepackage{natbib}
% \usepackage{hyperref}

\bibliographystyle{apalike}

\author{Titouan CHRISTOPHE\\\small 0529190 - VUB MA2 SOFT}
\date{\today}
\title{Learning Dynamics assignment 3\\\small Reinforcement Learning}

\begin{document}
\maketitle

\section{Question 1: $n$-armed bandit}
\subsection{Expected value for different strategies}

\paragraph{Random}
In this strategy, all actions are pulled randomly, with the same probability. Therefore, the expected reward at each turn is

\begin{equation}
E[R] = \frac{2.3 + 2.1 + 1.5 + 1.3}{4} = 1.8
\label{eq:random}
\end{equation}


\paragraph{$\epsilon$-Greedy}
\subparagraph{With $\epsilon = 0$}
In this situation, the agent always chooses the action he thinks will provide the more rewards, and never explore randomly. This imply that the first action taken will be the only one taken during the whole game. In my implementation, the first action is choosed randomly (otherwise it would be always the first one), and therefore the expected value will be the mean of the choosen action reward.

\subparagraph{With $\epsilon > 0$}
In this situation, the agent takes a random action with probability $\epsilon$, and takes the best action the rest of the time. At the beginning, the agent will start with an expected value $E[G]$ somewhere between 1.8 and 2.3, and progressively reach its final value as he discovers new and better actions. The agent will therefore have a final expected reward of
\begin{equation}
E[G_{\epsilon}] = \epsilon \times E[R] + (1-\epsilon) \times 2.3
\label{eq:greedy}
\end{equation}

This gives us:
\begin{itemize}
  \item{$E[G_{0.1}] = 2.25$}
  \item{$E[G_{0.2}] = 2.19$}
\end{itemize}


\paragraph{$\tau$-Softmax}
In this situation, the agent chooses a random a action, and the probability of choosing each one is calculated as a Boltzmann distribution of its expected return. We therefore have the following average expected value, for $n$ actions with average rewards $\mu_1, ..., \mu_{n}$:

\begin{equation}
E[S_{\tau}] = \sum_{i=1}^{n} \mu_{i} \frac{e^{\mu_i/\tau}}{\sum_{j=1}^{n} e^{\mu_j/\tau}}
\label{eq:softmax}
\end{equation}

Which gives:
\begin{itemize}
  \item{$ E[S_{1}] = 1.96 $}
  \item{$ E[S_{0.1}] = 2.28 $}
\end{itemize}

\subparagraph{Remarks}
\begin{itemize}
  \item When $\tau$ is large, $e^{\mu/\tau}$, approaches 1 for any $\mu$. Therefore, when the system is warm (high computational temperature $\tau$), all actions will have (nearly) equal probabilities. On the other hand if $\tau$ is very small, it  will give more importance to the learned values.
  \item The same probability distribution happens at the beginning, when the estimation of $\mu_i$ by the agent has not been updated (they are initialized to zero).
\end{itemize}

\subsection{Simulation}
On the following figures, the upper left graph represent the average reward over 1000 agents acting in a $4$-armed bandit problem. Each curve represent a different strategy used over 1100 epochs. At its right, a boxplot presents the average reward over all epochs and all agents. A second boxplot below similarly indicates the average reward over the last 100 epochs, and therefore indicates how well the agents were performing after the learning phase. Below the main graph, there are 4 plots, 1 for each arm, indicating the frequency of this action for each strategy, using the same color code as above.

\subsubsection{With initial parameters}
\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_1.pdf}
  \caption{\label{fig:1.1} $\mu = (2.3, 2.1, 1.5, 1.3), \sigma = (0.9, 0.6, 0.4, 2)$}
\end{figure}

The experimental average reward for the random strategy is around 1.8 as in Equation \ref{eq:random}. The $\epsilon$-Greedy strategies for $\epsilon \neq 0$, and the 1-softmax reach their theoretical values as well.

\paragraph{0-Greedy}
In this special case, the agent will pick a random action first, then always play the best action he knows. Most of the time, it will be the first action it picked (because he assumes a return of 0 for the other actions); but in some cases, if he picks the fourth action, he may receive a negative reward, and therefore he will pick the first action (this is because we use \texttt{numpy.argmax}\footnote{https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.argmax.html: "In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned."}), because it has an expected return of 0, which is greater than a negative value. We therefore observe a better-than-random performance for this strategy.

\paragraph{0.1-Softmax}
Using a fixed low temperature, once the agent has picked the first action, the probability for this action will be very much higher than for the others. As an example, if the agent choose the fourth action, and has a reward equals to the average reward of this action, the probability for this action will become:

$$ \frac{e^{1.3/0.1}}{3 + e^{1.3/0.1}} > 0.99 $$

The agent will therefore be very unlikely to pick another action, and this get accentuated with actions with a higher reward. However, if at the first epoch he choosed a negative reward, he switches to another action, as the probability for this action will become very low (less than 1\%)

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_1-q.pdf}
  \caption{\label{fig:1.1q} Expected rewards for Figure \ref{fig:1.1}}
\end{figure}

On Figure \ref{fig:1.1q}, we observe the average expected value of each action, for the different strategies. This value is estimated as an arithmetic mean of all the rewards from this action. The dashed blue line indicates the actual reward mean.

The 0-Greedy and 0.1-Softmax have erronated expected values, because they have been averaged over several runs, where different agents always took the same action. We clearly see that nonzero Greedy strategies quickly have a good estimation of the best action, but the estimator for other actions take time to converge. This is even worse for Softmax strategies.

\subsubsection{With doubled standard deviation}
\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_2.pdf}
  \caption{\label{fig:1.2} $\mu = (2.3, 2.1, 1.5, 1.3), \sigma = (1.8, 1.2, 0.8, 4)$}
\end{figure}

On Figure \ref{fig:1.2}, we observe the same behaviours as in Figure \ref{fig:1.1}, however the agents take more time to have a good estimate of the best action. This also leads to a worse optimal action selection for the 0.1-Greedy algorithm, which was the best performer in the initial conditions. With the doubled standard deviation, the 0.2-Greedy strategy is not significantly worse than the 0.1-Greedy, and has a weaker preference for the second action over the first, which best fits the bandits rewards.

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_2-q.pdf}
  \caption{\label{fig:1.2q} Expected rewards for Figure \ref{fig:1.2}}
\end{figure}

A very interesting thing in Figure \ref{fig:1.2q} is that the 1-Softmax strategy quickly converge to the actual value of the best action (though it takes about 4 times longer than with the initial stddev), but for the other actions, it may actually underestimate them (see Action 3).

\subsection{Dynamic strategies}
In this section, we will look at dynamic strategies, where the selected action also depends on the actual epoch. In Figure \ref{fig:timevar}, we provide a graphic representation of the evolution of those dynamic parameters over time.

\begin{figure}[H]
  \center
  \includegraphics[width=.45\textwidth]{sde.pdf}
  \includegraphics[width=.45\textwidth]{ldt.pdf}
  \caption{\label{fig:timevar} Behaviour of the dynamic $\epsilon$ and $\tau$.}
\end{figure}

\subsubsection{With initial parameters}
\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_3_1.pdf}
  \caption{\label{fig:1.3.1} $\mu = (2.3, 2.1, 1.5, 1.3), \sigma = (0.9, 0.6, 0.4, 2)$}
\end{figure}

Figure \ref{fig:1.3.1} is the justification for 1100 epochs (instead of the 1000 proposed in the assignment). As the dynamic Greedy implementation decreases its temperature $\tau$, the expression $e^{Q/\tau}$ reaches a singularity as $\tau$ approaches 0. However, the linearly decreasing $\tau$ clearly indicates that at some point, the agent should no more explore, but rather only exploit. In \cite{Kapetanakis2005}, the authors give a minimal temperature to the system, by adding a constant to a decreasing exponential. In my implementation, I use a static threshold.

$$ \tau = 4 \times \frac{\max(1, 1000-t)}{1000} $$

An interesting thing to notice is how the agent keep mixing actions 1 and 2 (the 2 bests) while decreasing the use of action 3 and 4, and only definitely choose action 1 at the very end. Over the whole simulation, the Greedy implementation provide more reward, but the softmax reaches a better instantaneous reward at the end. The Greedy implementation would therefore be suited to an online algorithm that cannot be trained beforehand, while the Softmax version would provide better performance once it has been trained.

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_3_1-q.pdf}
  \caption{\label{fig:1.3.1q} Expected rewards for Figure \ref{fig:1.3.1}}
\end{figure}

On Figure \ref{fig:1.3.1q}, we see that the agent estimate converges even more quickly to the actual reward. In the dynamic Greedy, it translates into an immediate very good reward, but for the Greedy implementation, it will only be useful when the temperature decreasing period is nearly over.

\subsubsection{With doubled standard deviation}
\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_3_2.pdf}
  \caption{\label{fig:1.3.2} $\mu = (2.3, 2.1, 1.5, 1.3), \sigma = (1.8, 1.2, 0.8, 4)$}
\end{figure}

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_3_2-q.pdf}
  \caption{\label{fig:1.3.2q} Expected rewards for Figure \ref{fig:1.3.2}}
\end{figure}

In Figure \ref{fig:1.3.2} and \ref{fig:1.3.2q}, we see that the Softmax implementation still underestimates the fourth action, but the effect is much lower than with a fixed temperature. With the doubled variance, the agent takes more time to estimate the actual rewards, but the dynamic temperature clearly helps. However the dynamic Greedy has much more difficulties to converge to the actual reward value.

\section{Exercise 2}
\subsection{Simulations}
For the Climbing Game, I choosed to use the FQM heuristic, presented in \cite{Kapetanakis2005} with a temperature decaying Boltzman selection. In addition, the minimal temperature is also parametrizable. Therefore, we have the following parametrization:

\begin{itemize}
  \item $\tau_0$ which is the initial temperature (at epoch 0)
  \item $\tau_N$ ; the minimal temperature (at epoch $N$)
  \item $c$; the factor of maximum frequency expectation
  \item $s$; the decay factor
\end{itemize}

such that

$$\tau(t) = e^{-st}\tau_0 + \tau_N$$

and

$$EV(a) = Q(a) + c \times \frac{\max(Q(a))}{count(a)}$$

As the reward is stochastic, the absolute frequency of the maximum observed reward is always 1, therefore this formula is equivalent to the original FMQ. We then choose a random action at epoch $t$ with probability

$$SOFTMAX_{\tau(t)}(EV(a_t))$$

On the graphs below, we see the average reward for 1000 pairs of agents playing the climbing game. At each turn, they both receive the same stochastic reward, drawn from normal distributions. On the right boxplot, we compare the overall average reward for the different strategies. We compare 5 different strategies: 2 FMQ with different parametrizations, but with the same initial temperature of 10, and 3 fixed softmaxes.

\paragraph{Computing Q for each player}
The expected reward $Q$ for each action of each player is computed as the arithmetic mean of the rewards he got using this action for all the action of its opponent, weighted by the average times the opponent played the action.

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_2_sigma1.pdf}
  \caption{\label{fig:2.1} $(\sigma, \sigma_{0}, \sigma_{1}) = (0.2, 0.2, 0.2)$}
\end{figure}

On Figure \ref{fig:2.1}, we see that the FMQ heuristic allows to find the optimum very quickly, and then to stay all the time in this configuration. With a small expectation coefficient, but high decaying factor (red curve), most of the pairs reach the best configuration in the earliest epochs, then the remaining pairs discover the good equilibrium. After 5000 epochs, all the playing pairs reached the optimum. Using a high expectation coefficient $c$, but slower decay rate (green curve), the players first reach the suboptimal pair of actions $(a_2, b_2)$ with an expected reward of 7, then jump together to the optimum. According to \cite{Kapetanakis2005}, the $c$ coefficient does not influence that much on the outcome of the algorithm, and this is what we observe here. Here, both agents take a decision based on the average expected reward, and the maximum average reward per action; this is why they tend to discover the optimum very quickly.

If we have a constant temperature of 1 (blue curve), the agents get stucked in the lower right square: as the agents are considering only the average reward so far per action, they are unlikely to choose $a_1$ or $b_1$, as the expected payoff is low. If we further lower the fixed temperature (yellow and black curves), the system never evolves.

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_2_sigma2.pdf}
  \caption{\label{fig:2.2} $(\sigma, \sigma_{0}, \sigma_{1}) = (0.1, 0.4, 0.1)$}
\end{figure}

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_2_sigma3.pdf}
  \caption{\label{fig:2.3} $(\sigma, \sigma_{0}, \sigma_{1}) = (0.1, 0.1, 0.4)$}
\end{figure}

We see no behavioral difference in the two others standard deviation configurations, apart from the observed reward being more noisy. We formulate 2 hypothesis:

\begin{itemize}
  \item Our agent (let's call him Agent Smith, he is such a good performer) is very good at learning, and he legitimately beats the game
  \item Our implementation has a bug
\end{itemize}

\subsection{Open questions}
\subsubsection{How will the learning process change if we make the agents independent learners?}
In this configuration, the agents only compute their expected rewards on their single action, and only learn 3 values instead of 9 in our case. It would eventually converge to the optimum, but slower than with Joint Action Learners. However, this allow to play in games where the opponent action and reward is not known, and simplify asymetric strategies (where the two players play differently).

\subsubsection{How will the learning process change if we make the agents always select the action that according to them will yield the highest reward (assuming the other agent plays the best response)?}
This is the 0-Greedy strategy we observed in the first exercise. If they don't explore enough, they might not find the best response for the opponent, and miss the optimum.

\section{Code}
The code to run the simulation has been written in Python, using Numpy and Matplotlib. It is provided in the following files:

\begin{itemize}
  \item \textbf{strategies.py}: The different strategies: functions that given a Q state and the actual epoch, return the index of the action to perform
  \item \textbf{simulate.py}: Function to perform a single step of the game, a simulation (sequence of steps), and run many of them with different strategies in parallel.
  \item \textbf{local\_config.py} and \textbf{plot.py}: Functions to configure the simulation (interactive plot or write to file, number of runs to perform, ...)
  \item \textbf{ld3q\{1,2\}.py}: The specific code for Exercise 1 and 2 and their plots.
\end{itemize}

\bibliography{report}

\end{document}
