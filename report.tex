\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{float}
% \usepackage{natbib}
% \usepackage{hyperref}

\bibliographystyle{apalike}

\author{Titouan CHRISTOPHE\\\small 0529190 - VUB MA2 SOFT}
\date{\today}
\title{Learning Dynamics assignment 3\\\small Reinforcement Learning}

\begin{document}
\maketitle

\section{Question 1: $n$-armed bandit}
\subsection{Expected value for different strategies}

\paragraph{Random}
In this strategy, all actions are pulled randomly, with the same probability. Therefore, the expected reward at each turn is

\begin{equation}
E[R] = \frac{2.3 + 2.1 + 1.5 + 1.3}{4} = 1.8
\label{eq:random}
\end{equation}


\paragraph{$\epsilon$-Greedy}
\subparagraph{With $\epsilon = 0$}
In this situation, the agent always chooses the action he thinks will provide the more rewards, and never explore randomly. This imply that the first action taken will be the only one taken during the whole game. In my implementation, the first action is choosed randomly (otherwise it would be always the first one), and therefore the expected value will be the mean of the choosen action reward.

\subparagraph{With $\epsilon > 0$}
In this situation, the agent takes a random action with probability $\epsilon$, and takes the best action the rest of the time. At the beginning, the agent will start with an expected value $E[G]$ somewhere between 1.8 and 2.3, and progressively reach its final value as he discovers new and better actions. The agent will therefore have a final expected reward of
\begin{equation}
E[G_{\epsilon}] = \epsilon \times E[R] + (1-\epsilon) \times 2.3
\label{eq:greedy}
\end{equation}

This gives us:
\begin{itemize}
  \item{$E[G_{0.1}] = 2.25$}
  \item{$E[G_{0.2}] = 2.19$}
\end{itemize}


\paragraph{$\tau$-Softmax}
In this situation, the agent chooses a random a action, and the probability of choosing each one is calculated as a Boltzmann distribution of its expected return. We therefore have the following average expected value, for $n$ actions with average rewards $\mu_1, ..., \mu_{n}$:

\begin{equation}
E[S_{\tau}] = \sum_{i=1}^{n} \mu_{i} \frac{e^{\mu_i/\tau}}{\sum_{j=1}^{n} e^{\mu_j/\tau}}
\label{eq:softmax}
\end{equation}

Which gives:
\begin{itemize}
  \item{$ E[S_{1}] = 1.96 $}
  \item{$ E[S_{0.1}] = 2.28 $}
\end{itemize}

\subparagraph{Remarks}
\begin{itemize}
  \item When $\tau$ is large, $e^{\mu/\tau}$, approaches 1 for any $\mu$. Therefore, when the system is warm (high computational temperature $\tau$), all actions will have (nearly) equal probabilities. On the other hand if $\tau$ is very small, it  will give more importance to the learned values.
  \item The same probability distribution happens at the beginning, when the estimation of $\mu_i$ by the agent has not been updated (they are initialized to zero).
\end{itemize}

\subsection{Simulation}
On the following figures, the upper left graph represent the average reward over 1000 agents acting in a $4$-armed bandit problem. Each curve represent a different strategy used over 1100 epochs. At its right, a boxplot presents the average reward over all epochs and all agents. A second boxplot below similarly indicates the average reward over the last 100 epochs, and therefore indicates how well the agents were performing after the learning phase. Below the main graph, there are 4 plots, 1 for each arm, indicating the frequency of this action for each strategy, using the same color code as above.

\subsubsection{With initial parameters}
\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_1.pdf}
  \caption{\label{fig:1.1} $\mu = (2.3, 2.1, 1.5, 1.3), \sigma = (0.9, 0.6, 0.4, 2)$}
\end{figure}

The experimental average reward for the random strategy is around 1.8 as in Equation \ref{eq:random}. The $\epsilon$-Greedy strategies for $\epsilon \neq 0$, and the 1-softmax reach their theoretical values as well.

\paragraph{0-Greedy}
In this special case, the agent will pick a random action first, then always play the best action he knows. Most of the time, it will be the first action it picked (because he assumes a return of 0 for the other actions); but in some cases, if he picks the fourth action, he may receive a negative reward, and therefore he will pick the first action (this is because we use \texttt{numpy.argmax}\footnote{https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.argmax.html: "In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned."}), because it has an expected return of 0, which is greater than a negative value. We therefore observe a better-than-random performance for this strategy.

\paragraph{0.1-Softmax}
Using a fixed low temperature, once the agent has picked the first action, the probability for this action will be very much higher than for the others. As an example, if the agent choose the fourth action, and has a reward equals to the average reward of this action, the probability for this action will become:

$$ \frac{e^{1.3/0.1}}{3 + e^{1.3/0.1}} > 0.99 $$

The agent will therefore be very unlikely to pick another action, and this get accentuated with actions with a higher reward. However, if at the first epoch he choosed a negative reward, he switches to another action, as the probability for this action will become very low (less than 1\%)

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_1-q.pdf}
  \caption{\label{fig:1.1q} Expected rewards for Figure \ref{fig:1.1}}
\end{figure}

On Figure \ref{fig:1.1q}, we observe the average expected value of each action, for the different strategies. This value is estimated as an arithmetic mean of all the rewards from this action. The dashed blue line indicates the actual reward mean.

The 0-Greedy and 0.1-Softmax have erronated expected values, because they have been averaged over several runs, where different agents always took the same action. We clearly see that nonzero Greedy strategies quickly have a good estimation of the best action, but the estimator for other actions take time to converge. This is even worse for Softmax strategies.

\subsubsection{With doubled standard deviation}
\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_2.pdf}
  \caption{\label{fig:1.2} $\mu = (2.3, 2.1, 1.5, 1.3), \sigma = (1.8, 1.2, 0.8, 4)$}
\end{figure}

On Figure \ref{fig:1.2}, we observe the same behaviours as in Figure \ref{fig:1.1}, however the agents take more time to have a good estimate of the best action. This also leads to a worse optimal action selection for the 0.1-Greedy algorithm, which was the best performer in the initial conditions. With the doubled standard deviation, the 0.2-Greedy strategy is not significantly worse than the 0.1-Greedy, and has a weaker preference for the second action over the first, which best fits the bandits rewards.

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_2-q.pdf}
  \caption{\label{fig:1.2q} Expected rewards for Figure \ref{fig:1.2}}
\end{figure}

A very interesting thing in Figure \ref{fig:1.2q} is that the 1-Softmax strategy quickly converge to the actual value of the best action (though it takes about 4 times longer than with the initial stddev), but for the other actions, it may actually underestimate them (see Action 3).

\subsection{Dynamic strategies}
In this section, we will look at dynamic strategies, where the selected action also depends on the actual epoch. In Figure \ref{fig:timevar}, we provide a graphic representation of the evolution of those dynamic parameters over time.

\begin{figure}[H]
  \center
  \includegraphics[width=.45\textwidth]{sde.pdf}
  \includegraphics[width=.45\textwidth]{ldt.pdf}
  \caption{\label{fig:timevar} Behaviour of the dynamic $\epsilon$ and $\tau$.}
\end{figure}

\subsubsection{With initial parameters}
\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_3_1.pdf}
  \caption{\label{fig:1.3.1} $\mu = (2.3, 2.1, 1.5, 1.3), \sigma = (0.9, 0.6, 0.4, 2)$}
\end{figure}

Figure \ref{fig:1.3.1} is the justification for 1100 epochs (instead of the 1000 proposed in the assignment). As the dynamic Greedy implementation decreases its temperature $\tau$, the expression $e^{Q/\tau}$ reaches a singularity as $\tau$ approaches 0. However, the linearly decreasing $\tau$ clearly indicates that at some point, the agent should no more explore, but rather only exploit. In \cite{Kapetanakis2005}, the authors give a minimal temperature to the system, by adding a constant to a decreasing exponential. In my implementation, I use a static threshold.

$$ \tau = 4 \times \frac{\max(1, 1000-t)}{1000} $$

An interesting thing to notice is how the agent keep mixing actions 1 and 2 (the 2 bests) while decreasing the use of action 3 and 4, and only definitely choose action 1 at the very end. Over the whole simulation, the Greedy implementation provide more reward, but the softmax reaches a better instantaneous reward at the end. The Greedy implementation would therefore be suited to an online algorithm that cannot be trained beforehand, while the Softmax version would provide better performance once it has been trained.

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_3_1-q.pdf}
  \caption{\label{fig:1.3.1q} Expected rewards for Figure \ref{fig:1.3.1}}
\end{figure}

On Figure \ref{fig:1.3.1q}, we see that the agent estimate converges even more quickly to the actual reward. In the dynamic Greedy, it translates into an immediate very good reward, but for the Greedy implementation, it will only be useful when the temperature decreasing period is nearly over.

\subsubsection{With doubled standard deviation}
\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_3_2.pdf}
  \caption{\label{fig:1.3.2} $\mu = (2.3, 2.1, 1.5, 1.3), \sigma = (1.8, 1.2, 0.8, 4)$}
\end{figure}

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_1_3_2-q.pdf}
  \caption{\label{fig:1.3.2q} Expected rewards for Figure \ref{fig:1.3.2}}
\end{figure}

In Figure \ref{fig:1.3.2} and \ref{fig:1.3.2q}, we see that the Softmax implementation still underestimates the fourth action, but the effect is much lower than with a fixed temperature. With the doubled variance, the agent takes more time to estimate the actual rewards, but the dynamic temperature clearly helps. However the dynamic Greedy has much more difficulties to converge to the actual reward value.

\section{Exercise 2}
\subsection{Simulations}
\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_2_sigma1.pdf}
  \caption{\label{fig:2.1} $(\sigma, \sigma_{0}, \sigma_{1}) = (0.2, 0.2, 0.2)$}
\end{figure}

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_2_sigma2.pdf}
  \caption{\label{fig:2.2} $(\sigma, \sigma_{0}, \sigma_{1}) = (0.1, 0.4, 0.1)$}
\end{figure}

\begin{figure}[H]
  \center
  \includegraphics[width=\textwidth]{Ex_2_sigma3.pdf}
  \caption{\label{fig:2.3} $(\sigma, \sigma_{0}, \sigma_{1}) = (0.1, 0.1, 0.4)$}
\end{figure}

\subsection{}

\bibliography{report}

\end{document}
